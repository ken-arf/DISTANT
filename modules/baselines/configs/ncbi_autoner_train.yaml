# Configuration file for training model.

# data path
task_name: ncbi 

num_bio_labels:  1

corpus_dir: data/NCBI/conll.autoner

load_embedding: True
glove_embedding_path: data/glove.6B/glove.6B.200d.txt
#word2vec: modules/autoner/PubMed-and-PMC-w2v.bin
word2vec: modules/autoner/PubMed-w2v.bin

# model save dir
model_dir: experiments/ncbi/autoner/models
vocab_path: experiments/ncbi/autoner/vocab.pth

# bert model
seed: 22

ent2int:
    disease: 0
    others: 1

model_checkpoint: allenai/scibert_scivocab_uncased
# nn model
dropout_rate: 0.5
embedding_dim: 768
hidden_size: 100

# label num
class_num: 1

# gpu
gpu: 0 

# training
train_epochs: 100 
train_batch_size: 10 
valid_batch_size: 10 

# training lr
train_lr: 1e-4
train_scheduler: linear
train_num_warmup_steps: 10000 
max_patient_count: 5 

# restore model
restore_model: False
restore_model_path: experiments/ncbi/autoner/models/model_best_autoner.pth

